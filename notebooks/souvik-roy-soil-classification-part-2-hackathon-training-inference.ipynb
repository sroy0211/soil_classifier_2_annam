{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102966,"databundleVersionId":12412856,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß™ Soil Classification - Part 2 Challenge\n\n## üìå Introduction\n\nSoil identification plays a vital role in agriculture, land management, and environmental science. In this competition, the task is to **classify whether an image depicts soil or not**. This binary classification problem is a foundational step toward more complex soil-type identification systems and supports real-world applications like crop planning, geotechnical analysis, and environmental monitoring.\n\nGiven that the training data contains **only soil images**, this task is framed as a **One-Class Learning** problem. Specifically, the challenge is to build a robust model that can learn the distribution of soil images and correctly identify whether an unseen image belongs to the soil category or not.\n\n---\n\n## üéØ Objective\n\n- Develop a **binary classifier** to detect soil images based on their visual features.\n- Implement a solution using **One-Class SVM**, a powerful method trained only on positive (soil) examples.\n- Use pre-trained deep CNNs (**ResNet50** and **EfficientNet-B0**) to extract high-quality features from raw images.\n\n---\n\n## üìà Evaluation Metric\n\nThe official metric for this challenge is:\n\n> **F1-Score (Binary Classification)**\n\nThis score is the harmonic mean of **precision** and **recall**, which rewards models that balance both false positives and false negatives. A high F1-score ensures the model performs well at identifying true soil images without being misled by outliers.\n\n---\n\n## üß† Our Approach\n\n- üß† **Feature Extraction**: \n  - **ResNet50** and **EfficientNet-B0** pretrained on ImageNet.\n  - Use **global average pooling** to convert feature maps to embeddings.\n- üßº **Preprocessing**:\n  - Resize all images to **224√ó224**.\n  - Normalize pixel values and apply appropriate model-specific preprocessing.\n- üßÆ **Dimensionality Reduction**:\n  - Apply **StandardScaler** and **PCA** to compress the high-dimensional feature space.\n- ‚úÖ **Model Training**:\n  - Use a **One-Class SVM** to learn the distribution of soil image features.\n  - Predict whether a test image is an inlier (soil) or outlier (non-soil).\n- üì§ **Submission**:\n  - Generate predictions on the hidden test set.\n  - Submit a CSV file with binary labels (1 = soil, 0 = not soil).\n\n---\n\nLet‚Äôs build a robust one-class classifier and see if we can dig up the soil from the noise! üåçüßë‚Äçüåæ\n","metadata":{}},{"cell_type":"code","source":"# Importing the essential libraries\n\nimport numpy as np                    # For numerical operations\nimport pandas as pd                   # For data manipulation and CSV handling\nimport os                             # For directory and file operations\nimport matplotlib.pyplot as plt       # For visualization\nfrom PIL import Image                 # To handle image file reading\nfrom keras.preprocessing import image \nfrom keras.applications.resnet50 import ResNet50, preprocess_input \nfrom keras.applications import EfficientNetB0\nfrom keras.applications.efficientnet import preprocess_input as effnet_preprocess\nfrom scipy.stats import uniform\n\n\n# PyTorch and torchvision libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\n\n# Sklearn for evaluation metrics\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import svm\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import OneClassSVM\n\n\n# tqdm for progress bars\n\nfrom tqdm import tqdm\n\nimport copy  # For saving the best model\nimport time  # For tracking training time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:38:28.723425Z","iopub.execute_input":"2025-05-24T03:38:28.724135Z","iopub.status.idle":"2025-05-24T03:38:28.730151Z","shell.execute_reply.started":"2025-05-24T03:38:28.724109Z","shell.execute_reply":"2025-05-24T03:38:28.729332Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Check if GPU is available and use it; else fall back to CPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:27.831849Z","iopub.execute_input":"2025-05-24T02:32:27.832577Z","iopub.status.idle":"2025-05-24T02:32:27.893852Z","shell.execute_reply.started":"2025-05-24T02:32:27.832544Z","shell.execute_reply":"2025-05-24T02:32:27.893098Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set random seeds for reproducibility\n\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:30.080389Z","iopub.execute_input":"2025-05-24T02:32:30.080915Z","iopub.status.idle":"2025-05-24T02:32:30.089656Z","shell.execute_reply.started":"2025-05-24T02:32:30.080891Z","shell.execute_reply":"2025-05-24T02:32:30.089029Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Since the notebook was made in Kaggle, the only change is if the user wants to run this notebook in another place\n# would be changing the path below\n\n# Define paths to training and test folders\n\ntrain_dir = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\ntest_dir = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\n\n# Load the CSV files with training labels and test image IDs\n\ntrain_df = pd.read_csv('/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv')\ntest_df = pd.read_csv('/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:30.972878Z","iopub.execute_input":"2025-05-24T02:32:30.973413Z","iopub.status.idle":"2025-05-24T02:32:31.001821Z","shell.execute_reply.started":"2025-05-24T02:32:30.973389Z","shell.execute_reply":"2025-05-24T02:32:31.001330Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Preview training data\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:32.119992Z","iopub.execute_input":"2025-05-24T02:32:32.120299Z","iopub.status.idle":"2025-05-24T02:32:32.139214Z","shell.execute_reply.started":"2025-05-24T02:32:32.120277Z","shell.execute_reply":"2025-05-24T02:32:32.138625Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           image_id  label\n0  img_ed005410.jpg      1\n1  img_0c5ecd2a.jpg      1\n2  img_ed713bb5.jpg      1\n3  img_12c58874.jpg      1\n4  img_eff357af.jpg      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>img_ed005410.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>img_0c5ecd2a.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>img_ed713bb5.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>img_12c58874.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>img_eff357af.jpg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Checking the unique labels in the training CSV file\n\nlabel_counts = train_df['label'].value_counts()\n\nprint(\"Unique label counts in training data:\")\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:34.111696Z","iopub.execute_input":"2025-05-24T02:32:34.111971Z","iopub.status.idle":"2025-05-24T02:32:34.122676Z","shell.execute_reply.started":"2025-05-24T02:32:34.111950Z","shell.execute_reply":"2025-05-24T02:32:34.121914Z"}},"outputs":[{"name":"stdout","text":"Unique label counts in training data:\nlabel\n1    1222\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Preview testing data\n# Since there are only labels of soil images we need to find out how to train such that\n# the model detects non-soil images for test dataset\n\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:35.147570Z","iopub.execute_input":"2025-05-24T02:32:35.147838Z","iopub.status.idle":"2025-05-24T02:32:35.154686Z","shell.execute_reply.started":"2025-05-24T02:32:35.147818Z","shell.execute_reply":"2025-05-24T02:32:35.154040Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                               image_id\n0  6595f1266325552489c7d1635fafb88f.jpg\n1  4b614841803d5448b59e2c6ca74ea664.jpg\n2  ca30e008692a50638b43d944f46245c8.jpg\n3  6a9046a219425f7599729be627df1c1a.jpg\n4  97c1e0276d2d5c2f88dddbc87357611e.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6595f1266325552489c7d1635fafb88f.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4b614841803d5448b59e2c6ca74ea664.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ca30e008692a50638b43d944f46245c8.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6a9046a219425f7599729be627df1c1a.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97c1e0276d2d5c2f88dddbc87357611e.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Checking the training data info\n\ntrain_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:36.750965Z","iopub.execute_input":"2025-05-24T02:32:36.751591Z","iopub.status.idle":"2025-05-24T02:32:36.767580Z","shell.execute_reply.started":"2025-05-24T02:32:36.751567Z","shell.execute_reply":"2025-05-24T02:32:36.766983Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1222 entries, 0 to 1221\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   image_id  1222 non-null   object\n 1   label     1222 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 19.2+ KB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Checking testing data info\n\ntest_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:32:38.364741Z","iopub.execute_input":"2025-05-24T02:32:38.365092Z","iopub.status.idle":"2025-05-24T02:32:38.374886Z","shell.execute_reply.started":"2025-05-24T02:32:38.365061Z","shell.execute_reply":"2025-05-24T02:32:38.374074Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 967 entries, 0 to 966\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   image_id  967 non-null    object\ndtypes: object(1)\nmemory usage: 7.7+ KB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Defining image and batch size\n\nIMAGE_SIZE = (224, 224)\nBATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:33:54.200657Z","iopub.execute_input":"2025-05-24T02:33:54.200956Z","iopub.status.idle":"2025-05-24T02:33:54.204571Z","shell.execute_reply.started":"2025-05-24T02:33:54.200934Z","shell.execute_reply":"2025-05-24T02:33:54.203846Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Loading images with labels for training dataset\n\ndef load_images_with_labels(df, image_dir):\n    \"\"\"Load images and their corresponding labels from dataframe\"\"\"\n    images = []\n    labels = []\n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        img_path = os.path.join(image_dir, row['image_id'])\n        try:\n            img = image.load_img(img_path, target_size=IMAGE_SIZE)\n            img_array = image.img_to_array(img)\n            images.append(img_array)\n            labels.append(row['label'])\n        except Exception as e:\n            print(f\"Skipping {row['image_id']} - error: {str(e)}\")\n    return np.array(images), np.array(labels)\n\nprint(\"Loading training images...\")\nX_all, y_all = load_images_with_labels(train_df, train_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:36:35.467166Z","iopub.execute_input":"2025-05-24T02:36:35.468141Z","iopub.status.idle":"2025-05-24T02:36:50.089036Z","shell.execute_reply.started":"2025-05-24T02:36:35.468116Z","shell.execute_reply":"2025-05-24T02:36:50.088246Z"}},"outputs":[{"name":"stdout","text":"Loading training images...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1222/1222 [00:14<00:00, 85.26it/s] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Assuming y_all contains only 1s (soil images) since it's one-class learning\n\n\nprint(\"Unique labels in y_all:\", np.unique(y_all))  # Should output: [1]\n\n# Get all soil indices (since we have no non-soil in training)\n\nsoil_indices = np.arange(len(y_all))  # All indices are soil\n\n# Split soil images into train (80%), val (10%), test (10%)\n\nsoil_train_idx, soil_temp_idx = train_test_split(\n    soil_indices, test_size=0.2, random_state=42\n)\nsoil_val_idx, soil_test_idx = train_test_split(\n    soil_temp_idx, test_size=0.5, random_state=42\n)\n\nprint(f\"\"\"\nData splits:\n- Training: {len(soil_train_idx)} samples\n- Validation: {len(soil_val_idx)} samples\n- Test: {len(soil_test_idx)} samples\n\"\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:42:01.172010Z","iopub.execute_input":"2025-05-24T02:42:01.172645Z","iopub.status.idle":"2025-05-24T02:42:01.179697Z","shell.execute_reply.started":"2025-05-24T02:42:01.172621Z","shell.execute_reply":"2025-05-24T02:42:01.178969Z"}},"outputs":[{"name":"stdout","text":"Unique labels in y_all: [1]\n\nData splits:\n- Training: 977 samples\n- Validation: 122 samples\n- Test: 123 samples\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Create datasets (X_train will contain only soil)\n\nX_train = X_all[soil_train_idx]\ny_train = y_all[soil_train_idx]  # Will be all 1s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:42:58.501627Z","iopub.execute_input":"2025-05-24T02:42:58.502330Z","iopub.status.idle":"2025-05-24T02:42:58.673347Z","shell.execute_reply.started":"2025-05-24T02:42:58.502304Z","shell.execute_reply":"2025-05-24T02:42:58.672435Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Validation and test sets (also only soil in this case)\n\nX_val = X_all[soil_val_idx]\ny_val = y_all[soil_val_idx]  # All 1s\n\nX_test = X_all[soil_test_idx]\ny_test = y_all[soil_test_idx]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:48:18.790964Z","iopub.execute_input":"2025-05-24T02:48:18.791571Z","iopub.status.idle":"2025-05-24T02:48:18.841303Z","shell.execute_reply.started":"2025-05-24T02:48:18.791546Z","shell.execute_reply":"2025-05-24T02:48:18.840564Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Preprocess images for ResNet\n\ndef preprocess_images(images):\n    return preprocess_input(images.copy())\n\nprint(\"Preprocessing images...\")\nX_train_preprocessed = preprocess_images(X_train)\nX_val_preprocessed = preprocess_images(X_val)\nX_test_preprocessed = preprocess_images(X_test)\n\n# Feature extraction with ResNet-50\n\ndef extract_features(images, batch_size=32):\n    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n    num_images = len(images)\n    features = []\n    \n    for i in tqdm(range(0, num_images, batch_size)):\n        batch = images[i:i+batch_size]\n        batch_features = base_model.predict(batch)\n        features.append(batch_features)\n    \n    return np.concatenate(features)\n\nprint(\"Extracting features from training set...\")\nX_train_features = extract_features(X_train_preprocessed)\n\nprint(\"Extracting features from validation set...\")\nX_val_features = extract_features(X_val_preprocessed)\n\nprint(\"Extracting features from test set...\")\nX_test_features = extract_features(X_test_preprocessed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:50:49.579927Z","iopub.execute_input":"2025-05-24T02:50:49.580241Z","iopub.status.idle":"2025-05-24T02:51:31.841699Z","shell.execute_reply.started":"2025-05-24T02:50:49.580189Z","shell.execute_reply":"2025-05-24T02:51:31.841008Z"}},"outputs":[{"name":"stdout","text":"Preprocessing images...\nExtracting features from training set...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1748055050.231651      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/31 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1748055055.885187      82 service.cc:148] XLA service 0x7c0d88002f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1748055055.886255      82 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1748055056.525882      82 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1748055059.625480      82 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n  3%|‚ñé         | 1/31 [00:06<03:14,  6.47s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","output_type":"stream"},{"name":"stderr","text":"  6%|‚ñã         | 2/31 [00:06<01:19,  2.76s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 10%|‚ñâ         | 3/31 [00:06<00:43,  1.57s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 13%|‚ñà‚ñé        | 4/31 [00:06<00:27,  1.00s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 16%|‚ñà‚ñå        | 5/31 [00:07<00:17,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 19%|‚ñà‚ñâ        | 6/31 [00:07<00:12,  1.99it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 23%|‚ñà‚ñà‚ñé       | 7/31 [00:07<00:09,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 26%|‚ñà‚ñà‚ñå       | 8/31 [00:07<00:07,  3.27it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 29%|‚ñà‚ñà‚ñâ       | 9/31 [00:07<00:05,  3.89it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 32%|‚ñà‚ñà‚ñà‚ñè      | 10/31 [00:07<00:04,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 35%|‚ñà‚ñà‚ñà‚ñå      | 11/31 [00:07<00:04,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 39%|‚ñà‚ñà‚ñà‚ñä      | 12/31 [00:08<00:03,  5.48it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n","output_type":"stream"},{"name":"stderr","text":" 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 13/31 [00:08<00:03,  5.82it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 14/31 [00:08<00:02,  6.22it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 15/31 [00:08<00:02,  6.54it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 16/31 [00:08<00:02,  6.75it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n","output_type":"stream"},{"name":"stderr","text":" 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 17/31 [00:08<00:02,  6.61it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 18/31 [00:08<00:01,  6.73it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 19/31 [00:09<00:01,  6.88it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 20/31 [00:09<00:01,  7.01it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 21/31 [00:09<00:01,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 22/31 [00:09<00:01,  7.16it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 23/31 [00:09<00:01,  7.04it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 24/31 [00:09<00:01,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 25/31 [00:09<00:00,  7.07it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 26/31 [00:10<00:00,  6.99it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n","output_type":"stream"},{"name":"stderr","text":" 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 27/31 [00:10<00:00,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 28/31 [00:10<00:00,  7.10it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 29/31 [00:10<00:00,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n","output_type":"stream"},{"name":"stderr","text":" 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 [00:10<00:00,  7.06it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:16<00:00,  1.93it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting features from validation set...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/4 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n","output_type":"stream"},{"name":"stderr","text":" 25%|‚ñà‚ñà‚ñå       | 1/4 [00:04<00:13,  4.47s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n","output_type":"stream"},{"name":"stderr","text":" 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:04<00:03,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","output_type":"stream"},{"name":"stderr","text":" 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.56s/it]","output_type":"stream"},{"name":"stdout","text":"Extracting features from test set...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/4 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","output_type":"stream"},{"name":"stderr","text":" 25%|‚ñà‚ñà‚ñå       | 1/4 [00:04<00:13,  4.62s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:04<00:03,  2.00s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:10<00:00,  2.59s/it]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ====== 2. NEW FEATURE FUSION SECTION ======\n\ndef extract_effnet_features(images, batch_size=32):\n    \"\"\"Extract EfficientNet features\"\"\"\n    base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n    num_images = len(images)\n    features = []\n    \n    for i in tqdm(range(0, num_images, batch_size)):\n        batch = effnet_preprocess(images[i:i+batch_size].copy())\n        batch_features = base_model.predict(batch, verbose=0)\n        features.append(batch_features)\n    \n    return np.concatenate(features)\n\nprint(\"\\nExtracting EfficientNet features...\")\nX_train_effnet = extract_effnet_features(X_train_preprocessed)\nX_val_effnet = extract_effnet_features(X_val_preprocessed)\nX_test_effnet = extract_effnet_features(X_test_preprocessed)\nX_competition_effnet = extract_effnet_features(X_competition_test_preprocessed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:34:08.546519Z","iopub.execute_input":"2025-05-24T03:34:08.547307Z","iopub.status.idle":"2025-05-24T03:35:21.320125Z","shell.execute_reply.started":"2025-05-24T03:34:08.547278Z","shell.execute_reply":"2025-05-24T03:35:21.319495Z"}},"outputs":[{"name":"stdout","text":"\nExtracting EfficientNet features...\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:19<00:00,  1.56it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:15<00:00,  3.85s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:15<00:00,  3.76s/it]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:18<00:00,  1.72it/s]\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Standardization and PCA\n\nprint(\"Applying standardization and PCA...\")\nss = StandardScaler()\nss.fit(X_train_features)\nX_train_scaled = ss.transform(X_train_features)\nX_test_scaled = ss.transform(X_test_features)\n\npca = PCA(n_components=512, whiten=True)\npca.fit(X_train_scaled)\nprint(f'Explained variance: {sum(pca.explained_variance_ratio_):.2f}')\n\nX_train_pca = pca.transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:54:35.010011Z","iopub.execute_input":"2025-05-24T02:54:35.010340Z","iopub.status.idle":"2025-05-24T02:54:35.781716Z","shell.execute_reply.started":"2025-05-24T02:54:35.010316Z","shell.execute_reply":"2025-05-24T02:54:35.781016Z"}},"outputs":[{"name":"stdout","text":"Applying standardization and PCA...\nExplained variance: 1.00\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# One-Class SVM Training\n\nprint(\"\\nTraining One-Class SVM...\")\noc_svm = svm.OneClassSVM(gamma='scale', kernel='rbf', nu=0.05)\noc_svm.fit(X_train_pca)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T02:54:52.086786Z","iopub.execute_input":"2025-05-24T02:54:52.087085Z","iopub.status.idle":"2025-05-24T02:54:52.149171Z","shell.execute_reply.started":"2025-05-24T02:54:52.087062Z","shell.execute_reply":"2025-05-24T02:54:52.148569Z"}},"outputs":[{"name":"stdout","text":"\nTraining One-Class SVM...\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"OneClassSVM(nu=0.05)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneClassSVM(nu=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneClassSVM</label><div class=\"sk-toggleable__content\"><pre>OneClassSVM(nu=0.05)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# Now we will load test images \n\ndef load_competition_test_images(test_df, test_dir):\n    \"\"\"Load all competition test images\"\"\"\n    test_images = []\n    for img_id in tqdm(test_df['image_id'], desc=\"Loading Competition Test Images\"):\n        img_path = os.path.join(test_dir, img_id)\n        try:\n            img = image.load_img(img_path, target_size=IMAGE_SIZE)\n            img_array = image.img_to_array(img)\n            test_images.append(img_array)\n        except Exception as e:\n            print(f\"Skipping {img_id} - error: {str(e)}\")\n            # For competition, we can't skip images - raise error\n            raise ValueError(f\"Failed to load competition test image {img_id}\")\n    return np.array(test_images)\n\nprint(\"\\nLoading competition test images...\")\nX_competition_test = load_competition_test_images(test_df, test_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:00:50.549241Z","iopub.execute_input":"2025-05-24T03:00:50.549555Z","iopub.status.idle":"2025-05-24T03:00:58.454560Z","shell.execute_reply.started":"2025-05-24T03:00:50.549532Z","shell.execute_reply":"2025-05-24T03:00:58.453975Z"}},"outputs":[{"name":"stdout","text":"\nLoading competition test images...\n","output_type":"stream"},{"name":"stderr","text":"Loading Competition Test Images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 967/967 [00:07<00:00, 125.90it/s]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Preprocess and extract features for competition test\n\nprint(\"Preprocessing competition test images...\")\nX_competition_test_preprocessed = preprocess_images(X_competition_test)\n\nprint(\"Extracting features from competition test set...\")\nX_competition_test_features = extract_features(X_competition_test_preprocessed)\n\n# Apply the same transformations\n\nprint(\"Transforming competition test features...\")\nX_competition_test_scaled = ss.transform(X_competition_test_features)\nX_competition_test_pca = pca.transform(X_competition_test_scaled)\n\n# Make predictions for competition\n\nprint(\"Making competition predictions...\")\ncompetition_preds = oc_svm.predict(X_competition_test_pca)\ncompetition_preds = np.where(competition_preds == 1, 1, 0)\n\n# Verify lengths\n\nprint(f\"Number of competition test images: {len(test_df)}\")\nprint(f\"Number of competition predictions: {len(competition_preds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:01:11.139795Z","iopub.execute_input":"2025-05-24T03:01:11.140082Z","iopub.status.idle":"2025-05-24T03:01:26.657722Z","shell.execute_reply.started":"2025-05-24T03:01:11.140060Z","shell.execute_reply":"2025-05-24T03:01:26.656861Z"}},"outputs":[{"name":"stdout","text":"Preprocessing competition test images...\nExtracting features from competition test set...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/31 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n","output_type":"stream"},{"name":"stderr","text":"  3%|‚ñé         | 1/31 [00:04<02:01,  4.06s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n","output_type":"stream"},{"name":"stderr","text":"  6%|‚ñã         | 2/31 [00:04<00:50,  1.76s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 10%|‚ñâ         | 3/31 [00:04<00:28,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n","output_type":"stream"},{"name":"stderr","text":" 13%|‚ñà‚ñé        | 4/31 [00:04<00:18,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n","output_type":"stream"},{"name":"stderr","text":" 16%|‚ñà‚ñå        | 5/31 [00:04<00:12,  2.06it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n","output_type":"stream"},{"name":"stderr","text":" 19%|‚ñà‚ñâ        | 6/31 [00:04<00:09,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n","output_type":"stream"},{"name":"stderr","text":" 23%|‚ñà‚ñà‚ñé       | 7/31 [00:04<00:07,  3.41it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n","output_type":"stream"},{"name":"stderr","text":" 26%|‚ñà‚ñà‚ñå       | 8/31 [00:05<00:05,  4.09it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 29%|‚ñà‚ñà‚ñâ       | 9/31 [00:05<00:04,  4.65it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n","output_type":"stream"},{"name":"stderr","text":" 32%|‚ñà‚ñà‚ñà‚ñè      | 10/31 [00:05<00:04,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 35%|‚ñà‚ñà‚ñà‚ñå      | 11/31 [00:05<00:03,  5.61it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","output_type":"stream"},{"name":"stderr","text":" 39%|‚ñà‚ñà‚ñà‚ñä      | 12/31 [00:05<00:03,  6.03it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 13/31 [00:05<00:02,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n","output_type":"stream"},{"name":"stderr","text":" 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 14/31 [00:05<00:02,  6.26it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 15/31 [00:06<00:02,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 16/31 [00:06<00:02,  6.19it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n","output_type":"stream"},{"name":"stderr","text":" 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 17/31 [00:06<00:02,  6.36it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","output_type":"stream"},{"name":"stderr","text":" 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 18/31 [00:06<00:01,  6.56it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 19/31 [00:06<00:01,  6.59it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n","output_type":"stream"},{"name":"stderr","text":" 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 20/31 [00:06<00:01,  6.62it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","output_type":"stream"},{"name":"stderr","text":" 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 21/31 [00:07<00:01,  6.79it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 22/31 [00:07<00:01,  6.76it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 23/31 [00:07<00:01,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 24/31 [00:07<00:01,  6.83it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 25/31 [00:07<00:00,  6.96it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 26/31 [00:07<00:00,  6.81it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n","output_type":"stream"},{"name":"stderr","text":" 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 27/31 [00:07<00:00,  6.76it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n","output_type":"stream"},{"name":"stderr","text":" 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 28/31 [00:08<00:00,  6.75it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n","output_type":"stream"},{"name":"stderr","text":" 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 29/31 [00:08<00:00,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n","output_type":"stream"},{"name":"stderr","text":" 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 [00:08<00:00,  7.01it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:13<00:00,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"Transforming competition test features...\nMaking competition predictions...\nNumber of competition test images: 967\nNumber of competition predictions: 967\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Final Submission Preparation\n\nprint(\"\\nFinalizing competition submission...\")\n\n# 1. Verify predictions are binary (0 or 1)\n\nassert set(competition_preds).issubset({0, 1}), \"Predictions contain invalid values\"\n\n# 2. Check class distribution\n\nprint(\"Prediction distribution:\")\nprint(pd.Series(competition_preds).value_counts())\n\n# 3. Create submission DataFrame with proper ordering\n\nsubmission_df = pd.DataFrame({\n    'image_id': test_df['image_id'],\n    'label': competition_preds\n})\n\n# 4. Verify no missing values\n\nassert not submission_df.isnull().any().any(), \"Submission contains missing values\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:02:19.747071Z","iopub.execute_input":"2025-05-24T03:02:19.747650Z","iopub.status.idle":"2025-05-24T03:02:19.755295Z","shell.execute_reply.started":"2025-05-24T03:02:19.747619Z","shell.execute_reply":"2025-05-24T03:02:19.754418Z"}},"outputs":[{"name":"stdout","text":"\nFinalizing competition submission...\nPrediction distribution:\n0    722\n1    245\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# 5. Save to CSV\n\nsubmission_file = 'submission_svm.csv'\nsubmission_df.to_csv(submission_file, index=False)\nprint(f\"\\nSubmission saved to {submission_file}\")\nprint(\"First 5 predictions:\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T03:04:16.395140Z","iopub.execute_input":"2025-05-24T03:04:16.395445Z","iopub.status.idle":"2025-05-24T03:04:16.408338Z","shell.execute_reply.started":"2025-05-24T03:04:16.395425Z","shell.execute_reply":"2025-05-24T03:04:16.407683Z"}},"outputs":[{"name":"stdout","text":"\nSubmission saved to submission_svm.csv\nFirst 5 predictions:\n                               image_id  label\n0  6595f1266325552489c7d1635fafb88f.jpg      1\n1  4b614841803d5448b59e2c6ca74ea664.jpg      1\n2  ca30e008692a50638b43d944f46245c8.jpg      0\n3  6a9046a219425f7599729be627df1c1a.jpg      1\n4  97c1e0276d2d5c2f88dddbc87357611e.jpg      1\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"---\n\n## üèÅ Results\n\nAfter training and fine-tuning our One-Class SVM pipeline with deep CNN-based feature extraction, we evaluated the model on the competition's test set.\n\n### üìä Prediction Distribution\n\nThe distribution of predicted labels on the test set:\n\n- **Soil (1)**: 245 images  \n- **Not Soil (0)**: 722 images\n\nThis indicates that the model is **conservative in labeling an image as soil**, which aligns with the nature of One-Class models where the goal is to detect deviations from the known class (soil).\n\n### üèÜ Leaderboard Performance\n\n> **Final F1-Score**: `0.855`\n\nOur model achieved an impressive **0.855 F1-score** on the public leaderboard, reflecting its strong balance between precision and recall for detecting soil images.\n\n---\n\nThe results validate that **deep feature representations + One-Class SVM** form an effective strategy for binary soil image classification ‚Äî especially when only positive-class examples are available during training.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}